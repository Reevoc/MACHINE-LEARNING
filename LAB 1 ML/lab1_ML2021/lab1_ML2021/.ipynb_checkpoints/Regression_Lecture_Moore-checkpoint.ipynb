{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Linear Regression on House Pricing Dataset\n",
    "We consider a reduced version of a dataset containing house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015.\n",
    "\n",
    "Link to the dataset.\n",
    "https://www.kaggle.com/harlfoxem/housesalesprediction\n",
    "\n",
    "For each house we know 18 house features (e.g., number of bedrooms, number of bathrooms, etc.) plus its price, that is what we would like to predict.\n",
    "\n",
    "A version of the dataset is in the ZIP file where you got this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put here your ``numero di matricola''\n",
    "ID_number = 1 # COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get in-line plots\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data\n",
    "Load the data from a .csv file\n",
    "\n",
    "**TO DO: insert your ID number (matricola)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
      "0     7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
      "1     6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
      "2     5631500400  20150225T000000  180000.0         2       1.00          770   \n",
      "3     2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
      "4     1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
      "...          ...              ...       ...       ...        ...          ...   \n",
      "3160  2759800110  20141031T000000  485000.0         3       2.50         1840   \n",
      "3161   579000096  20141010T000000  780000.0         3       1.50         1620   \n",
      "3162  9169600096  20140801T000000  720000.0         2       1.50         1840   \n",
      "3163  1226059112  20150220T000000  415000.0         3       1.00         1360   \n",
      "3164  6021503830  20140620T000000  480000.0         4       1.00         2080   \n",
      "\n",
      "      sqft_lot  floors  waterfront  view  ...  grade  sqft_above  \\\n",
      "0         5650     1.0           0     0  ...      7        1180   \n",
      "1         7242     2.0           0     0  ...      7        2170   \n",
      "2        10000     1.0           0     0  ...      6         770   \n",
      "3         5000     1.0           0     0  ...      7        1050   \n",
      "4         8080     1.0           0     0  ...      8        1680   \n",
      "...        ...     ...         ...   ...  ...    ...         ...   \n",
      "3160      8250     1.0           0     1  ...      8        1340   \n",
      "3161      7500     1.0           0     2  ...      8        1620   \n",
      "3162      9000     1.0           0     2  ...      8        1340   \n",
      "3163     73616     1.0           0     0  ...      7        1360   \n",
      "3164      5500     1.0           0     0  ...      7        1040   \n",
      "\n",
      "      sqft_basement  yr_built  yr_renovated  zipcode      lat     long  \\\n",
      "0                 0      1955             0    98178  47.5112 -122.257   \n",
      "1               400      1951          1991    98125  47.7210 -122.319   \n",
      "2                 0      1933             0    98028  47.7379 -122.233   \n",
      "3               910      1965             0    98136  47.5208 -122.393   \n",
      "4                 0      1987             0    98074  47.6168 -122.045   \n",
      "...             ...       ...           ...      ...      ...      ...   \n",
      "3160            500      1958             0    98177  47.7767 -122.378   \n",
      "3161              0      1949             0    98117  47.7014 -122.381   \n",
      "3162            500      1957             0    98136  47.5281 -122.388   \n",
      "3163              0      1971             0    98072  47.7528 -122.119   \n",
      "3164           1040      1941             0    98117  47.6838 -122.300   \n",
      "\n",
      "      sqft_living15  sqft_lot15  \n",
      "0            1340.0      5650.0  \n",
      "1            1690.0      7639.0  \n",
      "2            2720.0      8062.0  \n",
      "3            1360.0      5000.0  \n",
      "4            1800.0      7503.0  \n",
      "...             ...         ...  \n",
      "3160         1970.0      7920.0  \n",
      "3161         2440.0      7800.0  \n",
      "3162         1880.0      7560.0  \n",
      "3163         2040.0     50965.0  \n",
      "3164            NaN         NaN  \n",
      "\n",
      "[3165 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(ID_number)\n",
    "\n",
    "filename = \"kc_house_data.csv\"\n",
    "\n",
    "#load the data\n",
    "df = pd.read_csv(filename, sep = ',')\n",
    "\n",
    "#let's print out the data\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A quick overview of data\n",
    "\n",
    "Now let's clean the data and inspect it using the method describe()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.164000e+03</td>\n",
       "      <td>3.164000e+03</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3.164000e+03</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "      <td>3164.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.645240e+09</td>\n",
       "      <td>5.354358e+05</td>\n",
       "      <td>3.381163</td>\n",
       "      <td>2.071903</td>\n",
       "      <td>2070.027813</td>\n",
       "      <td>1.525054e+04</td>\n",
       "      <td>1.434893</td>\n",
       "      <td>0.009798</td>\n",
       "      <td>0.244311</td>\n",
       "      <td>3.459229</td>\n",
       "      <td>7.615676</td>\n",
       "      <td>1761.252212</td>\n",
       "      <td>308.775601</td>\n",
       "      <td>1967.489254</td>\n",
       "      <td>94.668774</td>\n",
       "      <td>98077.125158</td>\n",
       "      <td>47.557868</td>\n",
       "      <td>-122.212337</td>\n",
       "      <td>1982.544564</td>\n",
       "      <td>13176.302465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.854203e+09</td>\n",
       "      <td>3.809004e+05</td>\n",
       "      <td>0.895472</td>\n",
       "      <td>0.768212</td>\n",
       "      <td>920.251879</td>\n",
       "      <td>4.254457e+04</td>\n",
       "      <td>0.507792</td>\n",
       "      <td>0.098513</td>\n",
       "      <td>0.776298</td>\n",
       "      <td>0.682592</td>\n",
       "      <td>1.166324</td>\n",
       "      <td>815.934864</td>\n",
       "      <td>458.977904</td>\n",
       "      <td>28.095275</td>\n",
       "      <td>424.439427</td>\n",
       "      <td>54.172937</td>\n",
       "      <td>0.140789</td>\n",
       "      <td>0.139577</td>\n",
       "      <td>686.256670</td>\n",
       "      <td>25413.180755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000102e+06</td>\n",
       "      <td>7.500000e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>380.000000</td>\n",
       "      <td>6.490000e+02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>380.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1900.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98001.000000</td>\n",
       "      <td>47.177500</td>\n",
       "      <td>-122.514000</td>\n",
       "      <td>620.000000</td>\n",
       "      <td>660.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.199775e+09</td>\n",
       "      <td>3.150000e+05</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1430.000000</td>\n",
       "      <td>5.453750e+03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1190.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98032.000000</td>\n",
       "      <td>47.459575</td>\n",
       "      <td>-122.324250</td>\n",
       "      <td>1480.000000</td>\n",
       "      <td>5429.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.027701e+09</td>\n",
       "      <td>4.450000e+05</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1910.000000</td>\n",
       "      <td>8.000000e+03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1545.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1969.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98059.000000</td>\n",
       "      <td>47.572500</td>\n",
       "      <td>-122.226000</td>\n",
       "      <td>1830.000000</td>\n",
       "      <td>7873.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.358175e+09</td>\n",
       "      <td>6.402500e+05</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>1.122250e+04</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2150.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>1990.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98117.000000</td>\n",
       "      <td>47.680250</td>\n",
       "      <td>-122.124000</td>\n",
       "      <td>2360.000000</td>\n",
       "      <td>10408.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.839301e+09</td>\n",
       "      <td>5.350000e+06</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>8010.000000</td>\n",
       "      <td>1.651359e+06</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>6720.000000</td>\n",
       "      <td>2620.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>98199.000000</td>\n",
       "      <td>47.777600</td>\n",
       "      <td>-121.315000</td>\n",
       "      <td>5790.000000</td>\n",
       "      <td>425581.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id         price     bedrooms    bathrooms  sqft_living  \\\n",
       "count  3.164000e+03  3.164000e+03  3164.000000  3164.000000  3164.000000   \n",
       "mean   4.645240e+09  5.354358e+05     3.381163     2.071903  2070.027813   \n",
       "std    2.854203e+09  3.809004e+05     0.895472     0.768212   920.251879   \n",
       "min    1.000102e+06  7.500000e+04     0.000000     0.000000   380.000000   \n",
       "25%    2.199775e+09  3.150000e+05     3.000000     1.500000  1430.000000   \n",
       "50%    4.027701e+09  4.450000e+05     3.000000     2.000000  1910.000000   \n",
       "75%    7.358175e+09  6.402500e+05     4.000000     2.500000  2500.000000   \n",
       "max    9.839301e+09  5.350000e+06     8.000000     6.000000  8010.000000   \n",
       "\n",
       "           sqft_lot       floors   waterfront         view    condition  \\\n",
       "count  3.164000e+03  3164.000000  3164.000000  3164.000000  3164.000000   \n",
       "mean   1.525054e+04     1.434893     0.009798     0.244311     3.459229   \n",
       "std    4.254457e+04     0.507792     0.098513     0.776298     0.682592   \n",
       "min    6.490000e+02     1.000000     0.000000     0.000000     1.000000   \n",
       "25%    5.453750e+03     1.000000     0.000000     0.000000     3.000000   \n",
       "50%    8.000000e+03     1.000000     0.000000     0.000000     3.000000   \n",
       "75%    1.122250e+04     2.000000     0.000000     0.000000     4.000000   \n",
       "max    1.651359e+06     3.500000     1.000000     4.000000     5.000000   \n",
       "\n",
       "             grade   sqft_above  sqft_basement     yr_built  yr_renovated  \\\n",
       "count  3164.000000  3164.000000    3164.000000  3164.000000   3164.000000   \n",
       "mean      7.615676  1761.252212     308.775601  1967.489254     94.668774   \n",
       "std       1.166324   815.934864     458.977904    28.095275    424.439427   \n",
       "min       3.000000   380.000000       0.000000  1900.000000      0.000000   \n",
       "25%       7.000000  1190.000000       0.000000  1950.000000      0.000000   \n",
       "50%       7.000000  1545.000000       0.000000  1969.000000      0.000000   \n",
       "75%       8.000000  2150.000000     600.000000  1990.000000      0.000000   \n",
       "max      12.000000  6720.000000    2620.000000  2015.000000   2015.000000   \n",
       "\n",
       "            zipcode          lat         long  sqft_living15     sqft_lot15  \n",
       "count   3164.000000  3164.000000  3164.000000    3164.000000    3164.000000  \n",
       "mean   98077.125158    47.557868  -122.212337    1982.544564   13176.302465  \n",
       "std       54.172937     0.140789     0.139577     686.256670   25413.180755  \n",
       "min    98001.000000    47.177500  -122.514000     620.000000     660.000000  \n",
       "25%    98032.000000    47.459575  -122.324250    1480.000000    5429.500000  \n",
       "50%    98059.000000    47.572500  -122.226000    1830.000000    7873.000000  \n",
       "75%    98117.000000    47.680250  -122.124000    2360.000000   10408.250000  \n",
       "max    98199.000000    47.777600  -121.315000    5790.000000  425581.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove the data samples with missing values (NaN)\n",
    "df = df.dropna() \n",
    "\n",
    "df.describe()\n",
    "#for more interesting visualization: use Pandas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract input and output data. We want to predict the price by using features other than id as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data in training and test set\n",
    "\n",
    "Given $m$ total data, keep $m_t$ data as training data, and $m_{test}:=m - m_t$ for test data. For instance one can take $m_t=\\frac{3}{4}m $ of the data as training, and $m_{test}=\\frac{m}{4}$ as testing. Let us define\n",
    "- $S_{t}$ the training data set\n",
    "- $S_{test}$ the testing data set\n",
    "\n",
    "\n",
    "The reason for this splitting is as follows:\n",
    "\n",
    "TRAINING DATA: The training data are used to compute the empirical loss\n",
    "$$\n",
    "L_S(h) = \\frac{1}{m_t} \\sum_{z_i \\in S_{t}} \\ell(h,z_i)\n",
    "$$\n",
    "which is used to get $h_S$ in a given model class ${\\cal H}$.\n",
    "i.e. \n",
    "$$\n",
    "h_S = {\\rm arg\\; min}_{h \\in {\\cal H}} \\, L_S(h)\n",
    "$$\n",
    "\n",
    "TESTING DATA: Last, the test data set can be used to estimate the performance of the chosen hypothesis $h_{S}$ using:\n",
    "\n",
    "$$\n",
    "L_{\\cal D}(h_S)  \\simeq \\frac{1}{m_{test}} \\sum_{ z_i \\in S_{test}} \\ell(h_{S},z_i)\n",
    "$$\n",
    "\n",
    "**TO DO: split the data in training and test sets (suggestion: use $m_t=\\left\\lfloor\\frac{3}{4}m\\right\\rfloor $, $m_{test} = m-m_t$)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples:  3164\n",
      "Number of samples in training data:  2373\n",
      "Training input data size:  (2373, 18)\n",
      "Training output data size:  (2373,)\n",
      "Test input data size:  (791, 18)\n",
      "Test output data size:  (791,)\n"
     ]
    }
   ],
   "source": [
    "#let's consider only the values in the DataFrame\n",
    "Data = df.values\n",
    "\n",
    "# m = number of input samples\n",
    "m = Data.shape[0]\n",
    "\n",
    "print(\"Total number of samples: \", m)\n",
    "\n",
    "#size of training dataset\n",
    "size_training = int(3. * m / 4.)\n",
    "\n",
    "print(\"Number of samples in training data: \", size_training)\n",
    "\n",
    "#shuffle the data (to make sure we get a random split)\n",
    "\n",
    "np.random.shuffle(Data)\n",
    "\n",
    "#divide data into matrix X of features and target vector Y \n",
    "\n",
    "Y = Data[:,2]\n",
    "X = Data[:,3:]\n",
    "\n",
    "#training data\n",
    "\n",
    "X_training = X[:size_training, :]\n",
    "Y_training = Y[:size_training]\n",
    "print(\"Training input data size: \", X_training.shape)\n",
    "print(\"Training output data size: \", Y_training.shape)\n",
    "\n",
    "#test data, to be used to estimate the true loss of the final model\n",
    "X_test = X[size_training:, :]\n",
    "Y_test = Y[size_training:]\n",
    "print(\"Test input data size: \", X_test.shape)\n",
    "print(\"Test output data size: \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Normalization\n",
    "\n",
    "It is common practice in Statistics and Machine Learning to scale the data (= each variable) so that it is centered (zero mean) and has standard deviation equal to $1$. This helps in terms of numerical conditioning of the (inverse) problems of learning the model (the coefficients of the linear regression in this case), as well as to give the same scale to all the coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the training input data: [-2.14091048e-16  2.08851057e-16  1.19771215e-17  1.49714019e-17\n",
      "  1.01805533e-16 -2.50770982e-17 -5.53941872e-17 -8.15941405e-17\n",
      "  8.38398508e-17  1.31748337e-16  6.21313180e-17 -3.74883904e-15\n",
      "  1.79656823e-17  3.49275321e-14 -2.40279772e-14  4.26535241e-14\n",
      " -5.16513367e-17 -8.98284116e-18]\n",
      "\n",
      "Std of the training input data: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Mean of the test input data: [ 0.05622145  0.08092003  0.04010743  0.11708552  0.06840283 -0.01263497\n",
      " -0.04724761 -0.03730996  0.07540609  0.05607175 -0.01848504  0.08528822\n",
      "  0.04435727 -0.10545673 -0.07445634  0.08367153  0.01711166  0.05393013]\n",
      "\n",
      "Std of the test input data: [1.047132   0.99730186 0.9422277  2.31657231 1.01821348 0.93601148\n",
      " 0.90700288 0.96118092 1.01066589 0.97616387 0.95586542 1.00380641\n",
      " 1.09132291 0.96324839 1.03809331 1.00952186 0.97406047 1.27580071]\n"
     ]
    }
   ],
   "source": [
    "# scale the data: standardize the training feature matrix\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(X_training)\n",
    "X_training_scaled = scaler.transform(X_training)\n",
    "print(\"Mean of the training input data:\", X_training_scaled.mean(axis=0))\n",
    "print()\n",
    "print(\"Std of the training input data:\",X_training_scaled.std(axis=0))\n",
    "print()\n",
    "\n",
    "# now we scale the test feature matrix using the same transformation used\n",
    "# for the training dataset, since the weights of the model will be learned\n",
    "# data scaled according to such transformation\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "print(\"Mean of the test input data:\", X_test_scaled.mean(axis=0))\n",
    "print()\n",
    "print(\"Std of the test input data:\", X_test_scaled.std(axis=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training \n",
    "\n",
    "The model is trained minimizing the empirical error\n",
    "$$\n",
    "L_S(h) := \\frac{1}{N_t} \\sum_{z_i \\in S_{t}} \\ell(h,z_i)\n",
    "$$\n",
    "When the loss function is the quadratic loss\n",
    "$$\n",
    "\\ell(h,z) := (y - h(x))^2\n",
    "$$\n",
    "we define  the Residual Sum of Squares (RSS) as\n",
    "$$\n",
    "RSS(h):= \\sum_{z_i \\in S_{t}} \\ell(h,z_i) = \\sum_{z_i \\in S_{t}} (y_i - h(x_i))^2\n",
    "$$ so that the training error becomes\n",
    "$$\n",
    "L_S(h) = \\frac{RSS(h)}{m_t}\n",
    "$$\n",
    "\n",
    "For linear models we have $h(x) = <w,x>$ and the Empirical error $L_S(h)$ can be written\n",
    "in terms of the vector of parameters $w$ in the form\n",
    "$$\n",
    "L_S(w) = \\frac{1}{m_t} \\|Y - X w\\|^2\n",
    "$$\n",
    "where $Y$ and $X$ are the matrices whose $i-$th row are, respectively, the output data $y_i$ and the input vectors $x_i^\\top$.\n",
    "\n",
    "The least squares solution is given by the expression\n",
    "$$\n",
    "\\hat w = {\\rm arg\\;min}_w L_S(w) = (X^\\top X)^{-1} X^\\top Y\n",
    "$$\n",
    "When the matrix $X$ is not invertible (or even when it is invertible), the solution can be computed using the Moore-Penrose pseudonverse $(X^\\top X)^{\\dagger}$ of $(X^\\top X)$\n",
    "$$\n",
    "\\hat w = (X^\\top X)^{\\dagger} X^\\top Y\n",
    "$$\n",
    "The Moore-Penrose pseudoinverse $A^\\dagger$ of a matrix $A \\in \\mathbb{R}^{m\\times n}$ can be expressed in terms of the Singular Value Decomposition (SVD) as follows:\n",
    "let $A\\in \\mathbb{R}^{m\\times n}$ be of rank $r\\leq {\\rm min}(n,m)$ and let  \n",
    "$$\n",
    " A = USV^\\top\n",
    " $$\n",
    " be the singular value decomposition of  $A$ where  \n",
    " $$\n",
    " S = {\\rm diag}\\{s_1,s_2,..,s_r\\}\n",
    " $$\n",
    " Then \n",
    " $$\n",
    " A^\\dagger =V S^{-1} U^\\top \n",
    " $$\n",
    " \n",
    " In practice some of the singular values may be very small (e.g. $<1e-10$). Therefore it makes sense to  first approximate the matrix $A$ truncating the SVD and then using the pseudoinverse formula.\n",
    " \n",
    " More specifically, let us postulate that, given a threshold $T_h$ (e.g $=1e-12$), we have $\\sigma_i<T_h$, for $i=\\hat r + 1,..,r$. Then we can approximate (by SVD truncation) $A$ using:\n",
    " \n",
    " $$A = USV^\\top =U \\,{\\rm diag}\\{s_1,s_2,..,s_r\\}\\, V^\\top \\simeq \\hat A_r = U\\,{\\rm diag}\\{s_1,s_2,..,s_{\\hat r}, 0,..,0\\}\\,V^\\top\n",
    " $$\n",
    " So that \n",
    " $$\n",
    " A^\\dagger \\simeq \\hat A_r^\\dagger:= V \\,{\\rm diag}\\{1/s_1,1/s_2,..,1/s_{\\hat r}, 0,..,0\\}\\, U^\\top\n",
    " $$\n",
    "  \n",
    " In numpy, the Moore-Penrose pseudo-inverse of a matrix can be computed using the method numpy.linalg.pinv(...), which takes among its parameters the threshold for truncating the singular values to 0.\n",
    "  \n",
    " **TO DO: compute the linear regression coefficients according to the description above (using numpy.linalg.pinv(...) )**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LS coefficients by hand: [540320.2595870249 -38098.803493843545 29292.334936248808\n",
      " 88754.39194240556 -7166.575535523215 -3004.8599157764756\n",
      " 87307.01805661945 50021.25001592572 11855.3034363949 107808.92387367606\n",
      " 84481.2509960772 29124.756194696623 -76455.93042065795 12440.97970650568\n",
      " -32206.10576749511 81174.78763676251 -24343.349146152148\n",
      " 21873.871288944774 -11070.844281053505]\n",
      "RSS by hand: 104888598291172.03\n",
      "Empirical risk by hand: 44200842094.88918\n"
     ]
    }
   ],
   "source": [
    "#compute linear regression coefficients for training data\n",
    "\n",
    "#number of samples in the training set\n",
    "m_training = X_training_scaled.shape[0]\n",
    "\n",
    "#number of samples in the test set\n",
    "m_test = X_test_scaled.shape[0]\n",
    "\n",
    "# add a 1 at the beginning of each sample for training, and testing\n",
    "# the numpy function hstack is useful for such operation\n",
    "X_training_prime = np.hstack((np.ones((m_training,1)), X_training_scaled))\n",
    "\n",
    "X_test_prime = np.hstack((np.ones((m_test,1)), X_test_scaled))\n",
    "\n",
    "# set precision under which singular values are considered as zeros\n",
    "prec = 1e-10  \n",
    "\n",
    "# compute Moore-Penrose pseudoinverse of the matrix you need to compute \n",
    "# the weights of the model\n",
    "A_inv = np.linalg.pinv(np.dot(np.transpose(X_training_prime),X_training_prime) , prec)\n",
    "\n",
    "# now compute the weights and print them\n",
    "w_hand = np.dot(np.dot(A_inv, np.transpose(X_training_prime)), Y_training)\n",
    "\n",
    "print(\"LS coefficients by hand:\", w_hand)\n",
    "\n",
    "# compute Residual Sums of Squares by hand\n",
    "RSStr_hand = np.linalg.norm(Y_training -np.dot(X_training_prime, w_hand))**2\n",
    "\n",
    "# print the RSS\n",
    "print(\"RSS by hand:\",  RSStr_hand)\n",
    "\n",
    "# print the empirical risk\n",
    "print(\"Empirical risk by hand:\", RSStr_hand/m_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data prediction \n",
    "\n",
    "Compute the output predictions on both training and test set and compute the Residual Sum of Squares (RSS) defined above, the Empirical Loss and the quantity $R^2$ where\n",
    "$$\n",
    "R^2 = 1 - \\frac{\\sum_{z_i \\in S_t} (\\hat y_i - y_i)^2}{\\sum_{z_i \\in S_t} (y_i - \\bar y)^2} \\quad \\quad \\bar y = \\frac{1}{m_t} \\sum_{z_i \\in S_t} y_i\n",
    "$$\n",
    "is the so-called \"Coefficient of determination\" (COD).\n",
    "\n",
    "**TO DO Compute these quantities on  training and test data.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measure on Training Data (R^2): -32778893.687200367\n",
      "Measure on Test Data(R^2): 0.6596741626353212\n"
     ]
    }
   ],
   "source": [
    "#compute predictions on training and test\n",
    "prediction_training = np.dot(X_training_prime, w_hand)\n",
    "prediction_test = np.dot(X_test_prime, w_hand)\n",
    "\n",
    "#what about the RSS and empirical loss for points in the test data?\n",
    "#RSS_test = # COMPLETE\n",
    "\n",
    "#print(\"RSS on test data:\",  RSS_test)\n",
    "#print(\"Generalization error estimated on test data (i.e., empirical loss on test data):\", RSS_test/m_test)\n",
    "\n",
    "#another measure of how good our linear fit is given by the following (that is R^2)\n",
    "measure_training = 1- np.linalg.norm(Y_training - np.dot(X_training_prime, w_hand)**2 / np.linalg.norm(Y_training - Y_training.mean())**2)\n",
    "measure_test = 1- np.linalg.norm(Y_test - np.dot(X_test_prime, w_hand))**2 / np.linalg.norm(Y_test -Y_test.mean())**2\n",
    "\n",
    "print(\"Measure on Training Data (R^2):\", measure_training)\n",
    "print(\"Measure on Test Data(R^2):\", measure_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... and plot:\n",
    "\n",
    "\n",
    "### (1) output predictions on training  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions on Training data \n",
    "plt.figure()\n",
    "\n",
    "#the following is just for nice plotting, not required: it sorts the predictions by value so that they fall on\n",
    "# a line and it's easier to spot the differences\n",
    "sorting_permutation = sorted(range(len(prediction_training[0:m_training])), key=lambda k: prediction_training[0:m_training][k])\n",
    "plt.plot(Y_training[sorting_permutation], 'ko', alpha=0.5)\n",
    "plt.plot(prediction_training[sorting_permutation], 'rx')\n",
    "\n",
    "plt.xlabel('Input (index of instance)')\n",
    "plt.ylabel('Predicted Output')\n",
    "plt.title('Predictions on Training Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) output predictions on test  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions on test data \n",
    "plt.figure()\n",
    "\n",
    "# COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least-Squares using scikit-learn\n",
    "\n",
    "A fast way to compute the LS estimate is through sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the ``ones'' column in the features matrix (sklearn inserts it automatically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_training_OLS = X_training_scaled[:,1:]\n",
    "X_test_OLS = X_test_scaled[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "LinReg = linear_model.LinearRegression()  # build the object LinearRegression\n",
    "LinReg.fit(X_training_OLS, Y_training)  # estimate the LS coefficients\n",
    "print(\"Intercept:\", LinReg.intercept_)\n",
    "print(\"Least-Squares Coefficients:\", LinReg.coef_)\n",
    "prediction_training = LinReg.predict(X_training_OLS)  # predict output values on training set\n",
    "prediction_test = LinReg.predict(X_test_OLS)  # predict output values on test set\n",
    "print(\"Measure on training data:\", LinReg.score(X_training_OLS, Y_training))\n",
    "print(\"Measure on test data:\", LinReg.score(X_test_OLS, Y_test))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
